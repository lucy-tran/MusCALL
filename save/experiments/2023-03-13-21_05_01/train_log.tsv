# Trained with the MTG-735 dataset, with batch size = 7, learning rate = 5e-05
training size = 632, validation size = 103.

Epoch	train_loss	val_loss	metric	epoch_time	learing_rate	time_stamp
1	1.95464	1.89729	20.3883	354.235s	4.87764e-05	2023-03-13 21:10:57
2	1.8659	1.84332	23.301	353.158s	4.52254e-05	2023-03-13 21:16:53
3	1.80486	1.88017	17.4757	359.895s	3.96946e-05	2023-03-13 21:22:55
4	1.7582	1.8156	19.4175	365.179s	3.27254e-05	2023-03-13 21:29:01
5	1.71732	1.77052	26.2136	364.173s	2.5e-05	2023-03-13 21:35:06
6	1.73553	1.76682	24.2718	359.769s	1.72746e-05	2023-03-13 21:41:08
7	1.65257	1.74897	26.2136	404.993s	1.03054e-05	2023-03-13 21:47:54
8	1.68421	1.75666	19.4175	454.904s	4.77458e-06	2023-03-13 21:55:30
