model_config:
  model_name: muscall
  projection_dim: 512 # dimensionality of the multimodal projection layer
  temperature: null
  audio:
    model: ModifiedResNet # name of the audio backbone model (ModifiedResNet supported)
    pooling: attention # pooling mechanism to obtain fixed-size audio features. One of [average, attention]
    audio_len_seconds: 20 # max length in seconds
    hidden_size: 256 #
    conv_out_channels: 16 # number of output channels in the resnet conv layers
    n_mels: 128 # number of mel filterbanks to use in melspectrogram
    sample_rate: 16000 # sample rate of the input audio
    n_fft: 1024 # size of the FFT, creates n_fft // 2 + 1 bins.
    f_min: 0 # min frequency in the spectrogram
    f_max: 11025 # max frequency in the spectrogram
    ssl:
      do_ssl: false # whether to add audio self-supervised learning during pre-training
      ssl_loss_weight: 0.3
      ssl_temperature: 0.5
      ssl_projection_dim: 128
      p_polarity: 0.8
      p_noise: 0.3
      p_gain: 0.2
      p_pitch_shift: 0.4
  text:
    model: TextTransformer # name of the textual head model. One of TextTransformer, CLIPTextModel
    pretrained: openai/clip-vit-base-patch32 # name of the pretrained textual head model
    frozen_layers: null # range of layers to freeze during pretraining
    num_hidden_layers: 4 # number of hidden layers in the transformer
    hidden_size: 512 # dimensionality of the transformer layers
    num_attention_heads: 8 # number of attention heads in the transformer
    vocab_size: 49408 #
    max_position_embeddings: 77 # max number of tokens (seq is truncated if longer)
    attention_dropout: 0.2 # dropout probability for the attention weights
    dropout: 0.2 # dropout probaility in the feedforward blocks
  loss: clip # one of [clip, weighted_clip]
